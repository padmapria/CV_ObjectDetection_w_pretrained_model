{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"https://github.com/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb","timestamp":1720323882733}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"AVwdsB6BEY6w"},"source":["# Hands-on tutorial for DETR Object Detection with Transformers, run it in colab\n","\n","This notebook, is a reference to:\n","* use the pre-trained models to make object detection \n","# https://github.com/facebookresearch/detr?tab=readme-ov-file"]},{"cell_type":"markdown","metadata":{"id":"_GQzINI-FBWp"},"source":["## Preliminaries\n","This section contains the boilerplate necessary for the other sections. Run it first."]},{"cell_type":"code","metadata":{"id":"V1OPEeVOYTEV","executionInfo":{"status":"ok","timestamp":1720327045766,"user_tz":-480,"elapsed":9346,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["import math\n","\n","from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'retina'\n","\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","import torch\n","from torch import nn\n","from torchvision.models import resnet50\n","import torchvision.transforms as T\n","torch.set_grad_enabled(False);"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYVZjfGhYTEa","executionInfo":{"status":"ok","timestamp":1720327045767,"user_tz":-480,"elapsed":5,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["# COCO classes\n","CLASSES = [\n","    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n","    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n","    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n","    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n","    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n","    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n","    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n","    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n","    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n","    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n","    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n","    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n","    'toothbrush'\n","]\n","\n","# colors for visualization\n","COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n","          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ib4IT2yvYTEc","executionInfo":{"status":"ok","timestamp":1720327047907,"user_tz":-480,"elapsed":350,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["# standard PyTorch mean-std input image normalization\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Function to convert center-width-height format to xmin-ymin-xmax-ymax format (for output bounding box post-processing)\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","def rescale_bboxes(out_bbox, size):\n","    \"\"\"\n","    Rescale bounding boxes from normalized [0; 1] coordinates to image scales.\n","\n","    Parameters:\n","    out_bbox (torch.Tensor): Predicted bounding boxes in center-width-height format, shape (N, 4).\n","    size (tuple): Tuple containing width and height of the image.\n","\n","    Returns:\n","    torch.Tensor: Rescaled bounding boxes in xmin, ymin, xmax, ymax format, shape (N, 4).\n","    \"\"\"\n","    img_w, img_h = size  # Image width and height\n","\n","    # Convert center-width-height to xmin-ymin-xmax-ymax\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","\n","    # Scale bounding boxes to image dimensions\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","\n","    return b"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"miLNt3GKFSbt","executionInfo":{"status":"ok","timestamp":1720327066770,"user_tz":-480,"elapsed":2176,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["def plot_results(pil_img, prob, boxes):\n","    \"\"\"\n","    Plot the results of object detection on an image.\n","\n","    Parameters:\n","    pil_img (PIL.Image.Image): The input image in PIL format.\n","    prob (torch.Tensor): The probabilities or confidence scores for each detected object.\n","    boxes (torch.Tensor): The bounding boxes for each detected object, in the format (xmin, ymin, xmax, ymax).\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    names = []\n","    scores = []\n","    bbox_list = []\n","\n","    # Create a new figure with a specified size and Display the image\n","    plt.figure(figsize=(16, 10))\n","    plt.imshow(pil_img)\n","\n","    # Get the current axes instance on the current figure\n","    ax = plt.gca()\n","\n","    # Extend the colors list to ensure there are enough colors for all boxes\n","    colors = COLORS * 100\n","\n","    # Iterate over each probability, bounding box, and color\n","    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n","        # Add a rectangle patch to the axes for the bounding box\n","        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n","                                   fill=False, color=c, linewidth=3))\n","\n","        # Find the class with the highest probability\n","        cl = p.argmax()\n","\n","        # Prepare the text for the label with class name and probability\n","        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n","        scores.append(float(p[cl]))\n","        names.append(CLASSES[cl])\n","        bbox_list.append((xmin, ymin, xmax, ymax))\n","\n","        # Add the text label to the axes\n","        ax.text(xmin, ymin, text, fontsize=6,\n","                bbox=dict(facecolor='yellow', alpha=0.5))\n","\n","    print(\"Identified objects **\")\n","    for i in range(len(names)):\n","      print(f\"label: {names[i]}, score: {scores[i]}, bbox: {bbox_list[i]}\")\n","\n","    # Remove the axes for better visualization\n","    plt.axis('off')\n","\n","    # Show the plot\n","    plt.show()"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YdADNq60FZpr"},"source":["# Detection - using a pre-trained model from TorchHub\n","\n","To load the simplest model (DETR-R50) for fast inference from hub, run it on a custom image, and print the result. (any other model from the model zoo can be used)."]},{"cell_type":"code","metadata":{"id":"07IsWxj2YTEY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cee1f56-33fe-4d95-fd21-2bac802d2d80","executionInfo":{"status":"ok","timestamp":1720327072042,"user_tz":-480,"elapsed":3483,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n","model.eval();"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"markdown","metadata":{"id":"7tLVM0EtF5WH"},"source":["We now retrieve the image as a PIL image and apply some pre-processing, run it through the model"]},{"cell_type":"code","metadata":{"id":"ZRluxbQYYTEe","executionInfo":{"status":"ok","timestamp":1720327073455,"user_tz":-480,"elapsed":5,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["def load_predict(url):\n","  \"\"\"\n","  Load an image from URL, preprocess it, pass it through a model, and return outputs along with the original image.\n","\n","  Parameters:\n","  url (str): URL of the image to load.\n","\n","  Returns:\n","  torch.Tensor: Model outputs, containing pred_logits and pred_boxes.\n","  PIL.Image.Image: Original image loaded from URL.\n","  \"\"\"\n","  # Open and load the image from URL\n","  im = Image.open(requests.get(url, stream=True).raw)\n","\n","  # mean-std normalize the input image (batch-size: 1)\n","  img = transform(im).unsqueeze(0)\n","\n","  # propagate through the model , that returns outputs that contain\n","  # pred_logits (torch.Tensor): Raw class scores for each predicted box.\n","  #pred_boxes (torch.Tensor): Coordinates of the predicted bounding boxes.\n","  outputs = model(img)\n","\n","  return outputs, im"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["we filter the predictions. In particular, we keep only the objects for which the class confidence is higher than 0.4 (discounting the \"non-object\" predictions). You can lower this threshold if you want more predictions."],"metadata":{"id":"JUAjAzHaTUES"}},{"cell_type":"code","metadata":{"id":"hiV72KiKYTEi","executionInfo":{"status":"ok","timestamp":1720327105386,"user_tz":-480,"elapsed":352,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}}},"source":["# keep only predictions with 0.4+ confidence\n","def review_prediction(outputs,im,threshold=0.4):\n","  \"\"\"\n","  Review predictions by processing model outputs and plotting results.\n","\n","  Parameters:\n","  outputs (dict): Model outputs containing 'pred_logits' and 'pred_boxes'.\n","  im (PIL.Image.Image): Original image in PIL format.\n","\n","  Returns:\n","  None\n","  \"\"\"\n","  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","  keep = probas.max(-1).values > threshold\n","\n","  # convert boxes from [0; 1] to image scales\n","  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n","  plot_results(im, probas[keep], bboxes_scaled)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTAN1HT4eFA_"},"source":["That's it! Now try it on your own image and see what the self-attention of the Transformer Encoder learned by itself!\n"]},{"cell_type":"code","source":["url = \"https://images.data.gov.sg/api/traffic-images/2022/03/881b8734-cca2-49d2-844f-96f16e53a1ac.jpg\"\n","outputs, im = load_predict(url)\n","review_prediction(outputs,im)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ffAYOm4eY_2qGLB1Zf8psftBal_3xiJP"},"id":"rHM1B5YRb4x0","executionInfo":{"status":"ok","timestamp":1720327112247,"user_tz":-480,"elapsed":4829,"user":{"displayName":"deepika deepika","userId":"01147923396293761926"}},"outputId":"4f45ce98-d82f-40b8-f439-ff84ce599835"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["\n","# Conclusion\n","\n","In this notebook, we showed:\n","- how to use torchhub to compute predictions on your own image,"],"metadata":{"id":"dhcsUuSjVii9"}}]}
